{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217aa6dfaa2a41c6aad2c552ecfbba99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   8%|7         | 83.9M/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522f8fb4c8e44fea80660d658efa1a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a484f05a67ef4653bb888b9a8b955a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b76f8480242408c9ee470dd04e60234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620c26bbb10145c1b3b2bb75a529a5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c169e8e66c4f7b9d27ba44002e8fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# RAG (ChromaDB + SentenceTransformers) - No LangChain, No OpenAI\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "CSV_PATHS = [\n",
    "    \"./data/경기 (전부완료).csv\",\n",
    "    \"./data/광주 (전부완료).csv\",\n",
    "    \"./data/제주 (전부완료).csv\",\n",
    "    # 필요시 더 추가 or glob로 수집\n",
    "    # *중요*: 실제 경로/파일명에 맞게 수정\n",
    "]\n",
    "PERSIST_DIR = \"./chroma_travel_db\"         # ChromaDB 저장 폴더 (여기에 CSV 복사 X)\n",
    "COLLECTION_NAME = \"travel_places\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 인덱싱/질의 동일 모델 사용\n",
    "TOP_K = 5\n",
    "DEFAULT_REGION_FILTER: str | None = None   # 예: \"제주 (전부완료)\" 로 region 제한, 아니면 None\n",
    "\n",
    "# 문서 생성에 사용할 후보 컬럼들\n",
    "WANTED_COLS = [\n",
    "    \"place\",\"info\",\"주소\",\"이용시간\",\"휴일\",\"입장료\",\"주차\",\"화장실\",\n",
    "    \"체험\",\"체험가능 연령\",\"이용가능시설\",\"문의 및 안내\",\"홈페이지\"\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# 유틸\n",
    "# =========================\n",
    "def read_csv_safely(p: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(p, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(p, encoding=\"cp949\")\n",
    "\n",
    "def clean_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "\n",
    "def slug(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \"_\", s.strip())\n",
    "    s = re.sub(r\"[^0-9A-Za-z가-힣_\\-]\", \"\", s)\n",
    "    return s\n",
    "\n",
    "def row_to_text(row: pd.Series, exist_cols: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    한 장소(row) -> 하나의 '문서 문자열'\n",
    "    - place는 제목처럼 가장 앞\n",
    "    - 존재하는 컬럼만 '컬럼명: 값' 형태로 연결\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    place = clean_space(row.get(\"place\", \"\"))\n",
    "    if place:\n",
    "        parts.append(place)\n",
    "    for c in exist_cols:\n",
    "        if c == \"place\": \n",
    "            continue\n",
    "        v = str(row.get(c, \"\")).strip()\n",
    "        if v and v.lower() != \"nan\":\n",
    "            parts.append(f\"{c}: {clean_space(v)}\")\n",
    "    return \" / \".join(parts)\n",
    "\n",
    "# =========================\n",
    "# 임베딩 준비\n",
    "# =========================\n",
    "_embedder = SentenceTransformer(EMBED_MODEL)\n",
    "def embed_texts(texts: list[str]) -> list[list[float]]:\n",
    "    # normalize_embeddings=True 로 코사인 유사도와 상성 ↑\n",
    "    return _embedder.encode(texts, normalize_embeddings=True).tolist()\n",
    "\n",
    "# =========================\n",
    "# ChromaDB 초기화\n",
    "# =========================\n",
    "persist_dir = Path(PERSIST_DIR)\n",
    "persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "client = chromadb.PersistentClient(path=str(persist_dir))\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # 코사인 유사도\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 1) 여러 CSV 인덱싱(업서트)\n",
    "# =========================\n",
    "def ingest_csvs(paths: list[str | Path], batch: int = 256) -> int:\n",
    "    total = 0\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        if not p.exists():\n",
    "            print(f\"[경고] 파일 없음: {p}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = read_csv_safely(p)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] 읽기 실패 {p}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 잡열(예: Unnamed: 0) 제거\n",
    "        df = df.loc[:, ~df.columns.str.contains(r\"^Unnamed\")]\n",
    "        if \"place\" not in df.columns:\n",
    "            print(f\"[skip] {p.name}: 'place' 컬럼 없음\")\n",
    "            continue\n",
    "\n",
    "        df = df.dropna(subset=[\"place\"]).drop_duplicates(subset=[\"place\"])\n",
    "        exist_cols = [c for c in WANTED_COLS if c in df.columns]\n",
    "        region = p.stem  # 파일명(확장자 제거)을 메타 region에 저장\n",
    "\n",
    "        docs, metas, ids = [], [], []\n",
    "        for _, r in df.iterrows():\n",
    "            place = clean_space(r[\"place\"])\n",
    "            text = row_to_text(r, exist_cols)\n",
    "            if not text:\n",
    "                continue\n",
    "            doc_id = f\"{region}::{slug(place)}\"  # 파일명+장소로 고유화\n",
    "            docs.append(text)\n",
    "            metas.append({\n",
    "                \"place\": place,\n",
    "                \"region\": region,\n",
    "                \"source\": p.name,\n",
    "                \"address\": clean_space(r.get(\"주소\",\"\")),\n",
    "            })\n",
    "            ids.append(doc_id)\n",
    "\n",
    "        # 배치 임베딩 & 업서트\n",
    "        for i in tqdm(range(0, len(docs), batch), desc=f\"Ingest {region}\"):\n",
    "            chunk_docs = docs[i:i+batch]\n",
    "            chunk_ids  = ids[i:i+batch]\n",
    "            chunk_meta = metas[i:i+batch]\n",
    "            vecs = embed_texts(chunk_docs)\n",
    "            collection.upsert(documents=chunk_docs, embeddings=vecs, metadatas=chunk_meta, ids=chunk_ids)\n",
    "\n",
    "        total += len(ids)\n",
    "\n",
    "    print(f\"[완료] 인덱싱 문서 수: {total}\")\n",
    "    return total\n",
    "\n",
    "# =========================\n",
    "# 2) 검색 (질의 임베딩 → 유사도 검색)\n",
    "# =========================\n",
    "def retrieve(query: str, top_k: int = TOP_K, region: str | None = DEFAULT_REGION_FILTER):\n",
    "    qv = embed_texts([query])[0]\n",
    "    kwargs = dict(query_embeddings=[qv], n_results=top_k, include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    if region:\n",
    "        kwargs[\"where\"] = {\"region\": region}\n",
    "    res = collection.query(**kwargs)\n",
    "    docs = res[\"documents\"][0] if res.get(\"documents\") else []\n",
    "    metas = res[\"metadatas\"][0] if res.get(\"metadatas\") else []\n",
    "    dists = res[\"distances\"][0] if res.get(\"distances\") else []\n",
    "    return list(zip(docs, metas, dists))\n",
    "\n",
    "# =========================\n",
    "# 3) LLM 없이 '추출적' 답변 만들기\n",
    "#    - 규칙 기반 + 문장 임베딩 재랭킹으로 핵심 문장 골라 답변\n",
    "# =========================\n",
    "# 간단 문장 분할기(한국어/기호 기준)\n",
    "# 문장 분할 (한글 종결형/영문 구두점/ \" / \" ) 대응\n",
    "_SENT_DELIM = \"§§\"  # 문장 사이에만 잠깐 넣을 임시 구분자(본문에 나올 일 거의 없는 문자)\n",
    "\n",
    "def split_sentences(text: str) -> list[str]:\n",
    "    t = str(text)\n",
    "\n",
    "    # 1) 한글 종결 + 마침표 패턴 뒤에 구분자 삽입\n",
    "    #    예: \"다.\", \"요.\", \"죠.\", \"함.\"\n",
    "    t = re.sub(r\"(다\\.|요\\.|죠\\.|함\\.)\\s*\", r\"\\1\" + _SENT_DELIM, t)\n",
    "\n",
    "    # 2) 일반 영문 구두점(. ! ?) 뒤에도 구분자 삽입\n",
    "    t = re.sub(r\"([.!?])\\s*\", r\"\\1\" + _SENT_DELIM, t)\n",
    "\n",
    "    # 3) 네가 문서 생성 시 사용한 \" / \" 도 문장 경계로 취급\n",
    "    t = t.replace(\" / \", _SENT_DELIM)\n",
    "\n",
    "    # 4) 구분자로 split\n",
    "    sents = [s.strip() for s in t.split(_SENT_DELIM) if s.strip()]\n",
    "\n",
    "    # 5) 너무 짧은 토막 제거 (선택)\n",
    "    sents = [s for s in sents if len(s) >= 4]\n",
    "    return sents\n",
    "\n",
    "\n",
    "def detect_intents(query: str) -> list[str]:\n",
    "    q = query.lower()\n",
    "    hits = []\n",
    "    for field, kws in INTENT_KEYS.items():\n",
    "        for kw in kws:\n",
    "            if kw.lower() in q:\n",
    "                hits.append(field)\n",
    "                break\n",
    "    return hits\n",
    "\n",
    "def extract_field_lines(doc: str, desired_fields: list[str]) -> list[str]:\n",
    "    \"\"\"문서 문자열 내부에서 '필드명: 값' 형태 라인을 추출\"\"\"\n",
    "    lines = []\n",
    "    for field in desired_fields:\n",
    "        # '필드명: ' 로 시작하는 구간만 골라내기 (문서 생성 규칙과 매칭)\n",
    "        m = re.findall(fr\"{re.escape(field)}:\\s*([^/]+)\", doc)\n",
    "        for v in m:\n",
    "            v = clean_space(v)\n",
    "            if v:\n",
    "                lines.append(f\"{field}: {v}\")\n",
    "    return lines\n",
    "\n",
    "def answer_extractive(question: str, top_k: int = TOP_K, region: str | None = DEFAULT_REGION_FILTER, n_sent: int = 4) -> str:\n",
    "    # 1) 상위 문서 회수\n",
    "    hits = retrieve(question, top_k=top_k, region=region)\n",
    "    if not hits:\n",
    "        return \"관련 문서를 찾지 못했어요. 다른 키워드로 다시 물어봐 주세요.\"\n",
    "\n",
    "    # 2) 질의 의도 파악(주소/입장료/이용시간/주차 등)\n",
    "    fields = detect_intents(question)\n",
    "\n",
    "    # 3) 선택지 A: 필드가 있다면 우선 해당 필드 라인을 긁어온 뒤, 부족하면 문장 랭킹\n",
    "    picked_lines = []\n",
    "    if fields:\n",
    "        for doc, meta, _ in hits:\n",
    "            picked_lines += extract_field_lines(doc, fields)\n",
    "        picked_lines = list(dict.fromkeys(picked_lines))  # 중복 제거(순서 유지)\n",
    "\n",
    "    # 4) 선택지 B: (보강) 문장 임베딩 기반 랭킹으로 핵심 문장 추가\n",
    "    #    - 각 문서를 문장으로 쪼개고, 질의와 코사인 유사도 높은 순으로 n_sent개 뽑기\n",
    "    if len(picked_lines) < n_sent:\n",
    "        query_vec = np.array(embed_texts([question])[0])\n",
    "        cand_sents = []\n",
    "        sent_to_place = {}\n",
    "        for doc, meta, _ in hits:\n",
    "            place = meta.get(\"place\",\"\")\n",
    "            sents = split_sentences(doc)\n",
    "            if not sents: \n",
    "                continue\n",
    "            vecs = np.array(embed_texts(sents))\n",
    "            sims = (vecs @ query_vec)  # 이미 정규화돼 있으므로 내적=코사인\n",
    "            top_idx = sims.argsort()[::-1][:max(2, n_sent)]  # 각 문서에서 상위 몇 개\n",
    "            for i in top_idx:\n",
    "                sent = clean_space(sents[i])\n",
    "                if len(sent) >= 6:\n",
    "                    cand_sents.append((sims[i], sent, place))\n",
    "                    sent_to_place[sent] = place\n",
    "\n",
    "        # 전체 상위 n_sent~2n_sent 정도로 압축\n",
    "        cand_sents.sort(key=lambda x: x[0], reverse=True)\n",
    "        for _, sent, place in cand_sents:\n",
    "            if sent not in picked_lines:\n",
    "                picked_lines.append(sent)\n",
    "            if len(picked_lines) >= n_sent:\n",
    "                break\n",
    "\n",
    "    # 5) 답변 조립 (가이드 톤)\n",
    "    #    - 첫 줄: 가장 관련 높은 place 요약\n",
    "    top_places = [m.get(\"place\",\"\") for _, m, _ in hits if m.get(\"place\")]\n",
    "    top_places = [p for i,p in enumerate(top_places) if p and p not in top_places[:i]]\n",
    "    header = \"\"\n",
    "    if top_places:\n",
    "        header = f\"{top_places[0]} 관련 안내예요.\"\n",
    "\n",
    "    # 6) 본문: 필드 라인/핵심 문장 3~5줄\n",
    "    body_lines = picked_lines[:max(3, n_sent)]\n",
    "    if not body_lines:\n",
    "        # 그래도 비어있으면 info 필드만 추출\n",
    "        for doc, _, _ in hits:\n",
    "            m = re.findall(r\"info:\\s*([^/]+)\", doc)\n",
    "            if m:\n",
    "                body_lines.append(clean_space(m[0]))\n",
    "                if len(body_lines) >= 3:\n",
    "                    break\n",
    "\n",
    "    # 7) 출처 요약\n",
    "    sources = \", \".join(top_places[:5])\n",
    "\n",
    "    # 8) 최종 문자열\n",
    "    parts = []\n",
    "    if header: parts.append(header)\n",
    "    parts += body_lines\n",
    "    if sources:\n",
    "        parts.append(f\"(출처: {sources})\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "# =========================\n",
    "# 실행 예시\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) 최초 1회 인덱싱 (이미 했다면 주석 처리 가능)\n",
    "    # ingest_csvs(CSV_PATHS)\n",
    "\n",
    "    # 2) 검색 프리뷰\n",
    "    # for doc, meta, dist in retrieve(\"경주 불국사 입장료 알려줘\", region=None):\n",
    "    #     print(f\"[{meta.get('place')}] dist={dist:.4f}\\n{doc[:180]}...\\n\")\n",
    "\n",
    "    # 3) 과금 없이 '추출형' 답변\n",
    "    # print(answer_extractive(\"경주 불국사 입장료 알려줘\", region=None))\n",
    "    # print(answer_extractive(\"제주 비 오는 날 가기 좋은 데이트 코스 추천해줘\", region=\"제주 (전부완료)\"))\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aea01b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 모델: intfloat/multilingual-e5-base\n",
      "패치 적용 완료 ✅  — 새 컬렉션: travel_places_e5 | DB: ./chroma_travel_db_e5\n"
     ]
    }
   ],
   "source": [
    "# === (1) 새 모델 선택: e5/gte/bge 중 하나 ===\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 예) e5 권장\n",
    "EMBED_MODEL = \"intfloat/multilingual-e5-base\"\n",
    "# 예) gte 대안\n",
    "# EMBED_MODEL = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "# 예) bge-m3 (가장 무겁고 성능↑, 1024차원)\n",
    "# EMBED_MODEL = \"BAAI/bge-m3\"\n",
    "\n",
    "_embedder = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "# === (2) e5/gte/bge 권장 프롬프트 규칙 ===\n",
    "def embed_passages(texts):\n",
    "    # 문서(패시지) 임베딩\n",
    "    return _embedder.encode([f\"passage: {t}\" for t in texts], normalize_embeddings=True).tolist()\n",
    "\n",
    "def embed_query(q: str):\n",
    "    # 질의 임베딩\n",
    "    return _embedder.encode([f\"query: {q}\"], normalize_embeddings=True).tolist()[0]\n",
    "\n",
    "# (MiniLM 시절 함수가 남아 있어도 안전하게 우회)\n",
    "def embed_texts(texts):\n",
    "    # 과거 호출을 대비해 passage 규칙으로 라우팅\n",
    "    return embed_passages(texts)\n",
    "\n",
    "# === (3) 새 DB 폴더/컬렉션으로 분리 (차원 충돌 방지) ===\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "\n",
    "PERSIST_DIR = \"./chroma_travel_db_e5\"      # ★ 새 폴더 이름\n",
    "COLLECTION_NAME = \"travel_places_e5\"       # ★ 새 컬렉션 이름\n",
    "\n",
    "Path(PERSIST_DIR).mkdir(parents=True, exist_ok=True)\n",
    "_client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "collection = _client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\":\"cosine\"}\n",
    ")\n",
    "\n",
    "print(\"임베딩 모델:\", EMBED_MODEL)\n",
    "\n",
    "# === (4) ingest_csvs 내부 업서트 임베딩 교체 ===\n",
    "# 네 노트북에 이미 존재하는 ingest_csvs가 있다면, 아래 함수를 재정의(오버라이드)합니다.\n",
    "def ingest_csvs(paths: list[str | Path], batch: int = 256) -> int:\n",
    "    import pandas as pd, re\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    WANTED_COLS = [\n",
    "        \"place\",\"info\",\"주소\",\"이용시간\",\"휴일\",\"입장료\",\"주차\",\"화장실\",\n",
    "        \"체험\",\"체험가능 연령\",\"이용가능시설\",\"문의 및 안내\",\"홈페이지\"\n",
    "    ]\n",
    "\n",
    "    def read_csv_safely(p: Path):\n",
    "        try:\n",
    "            return pd.read_csv(p, encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            return pd.read_csv(p, encoding=\"cp949\")\n",
    "\n",
    "    def clean_space(s: str) -> str:\n",
    "        import re\n",
    "        return re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "\n",
    "    def slug(s: str) -> str:\n",
    "        import re\n",
    "        s = re.sub(r\"\\s+\", \"_\", s.strip())\n",
    "        s = re.sub(r\"[^0-9A-Za-z가-힣_\\-]\", \"\", s)\n",
    "        return s\n",
    "\n",
    "    def row_to_text(row, exist_cols):\n",
    "        parts = []\n",
    "        place = clean_space(row.get(\"place\",\"\"))\n",
    "        if place:\n",
    "            parts.append(place)\n",
    "        for c in exist_cols:\n",
    "            if c == \"place\": \n",
    "                continue\n",
    "            v = str(row.get(c,\"\")).strip()\n",
    "            if v and v.lower()!=\"nan\":\n",
    "                parts.append(f\"{c}: {clean_space(v)}\")\n",
    "        return \" / \".join(parts)\n",
    "\n",
    "    total = 0\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        if not p.exists():\n",
    "            print(f\"[경고] 파일 없음: {p}\")\n",
    "            continue\n",
    "        try:\n",
    "            df = read_csv_safely(p)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] 읽기 실패 {p}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df = df.loc[:, ~df.columns.str.contains(r\"^Unnamed\")]\n",
    "        if \"place\" not in df.columns:\n",
    "            print(f\"[skip] {p.name}: 'place' 컬럼 없음\")\n",
    "            continue\n",
    "\n",
    "        df = df.dropna(subset=[\"place\"]).drop_duplicates(subset=[\"place\"])\n",
    "        exist_cols = [c for c in WANTED_COLS if c in df.columns]\n",
    "        region = p.stem\n",
    "\n",
    "        docs, metas, ids = [], [], []\n",
    "        for _, r in df.iterrows():\n",
    "            place = clean_space(r[\"place\"])\n",
    "            text = row_to_text(r, exist_cols)\n",
    "            if not text: \n",
    "                continue\n",
    "            doc_id = f\"{region}::{slug(place)}\"\n",
    "            docs.append(text)\n",
    "            metas.append({\n",
    "                \"place\": place,\n",
    "                \"region\": region,\n",
    "                \"source\": p.name,\n",
    "                \"address\": clean_space(r.get(\"주소\",\"\")),\n",
    "            })\n",
    "            ids.append(doc_id)\n",
    "\n",
    "        for i in tqdm(range(0, len(docs), batch), desc=f\"Ingest {region}\"):\n",
    "            chunk_docs = docs[i:i+batch]\n",
    "            chunk_ids  = ids[i:i+batch]\n",
    "            chunk_meta = metas[i:i+batch]\n",
    "            vecs = embed_passages(chunk_docs)      # ★ 여기!\n",
    "            collection.upsert(documents=chunk_docs, embeddings=vecs,\n",
    "                              metadatas=chunk_meta, ids=chunk_ids)\n",
    "\n",
    "        total += len(ids)\n",
    "\n",
    "    print(f\"[완료] 인덱싱 문서 수: {total}\")\n",
    "    return total\n",
    "\n",
    "# === (5) retrieve 함수 내 질의 임베딩 교체 ===\n",
    "TOP_K = 5\n",
    "DEFAULT_REGION_FILTER = None  # 필요시 \"제주 (전부완료)\" 등으로 지정\n",
    "\n",
    "def retrieve(query: str, top_k: int = TOP_K, region: str | None = DEFAULT_REGION_FILTER):\n",
    "    qv = embed_query(query)  # ★ 여기!\n",
    "    kwargs = dict(query_embeddings=[qv], n_results=top_k, include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    if region:\n",
    "        kwargs[\"where\"] = {\"region\": region}\n",
    "    res = collection.query(**kwargs)\n",
    "    docs = res[\"documents\"][0] if res.get(\"documents\") else []\n",
    "    metas = res[\"metadatas\"][0] if res.get(\"metadatas\") else []\n",
    "    dists = res[\"distances\"][0] if res.get(\"distances\") else []\n",
    "    return list(zip(docs, metas, dists))\n",
    "\n",
    "# === (6) 추천/추출 함수들도 새 임베딩으로 교체 ===\n",
    "# split_sentences, extract_tags 등은 네가 이미 만든 그대로 사용\n",
    "\n",
    "def recommend_places(\n",
    "    query: str,\n",
    "    region: str | None = None,\n",
    "    k_docs: int = 20,\n",
    "    top_n: int = 5,\n",
    "    per_place_sent: int = 2,\n",
    "    w_sim: float = 0.85,\n",
    "    w_rule: float = 0.15,\n",
    "):\n",
    "    import numpy as np\n",
    "    from collections import defaultdict\n",
    "\n",
    "    hits = retrieve(query, top_k=k_docs, region=region)\n",
    "    if not hits:\n",
    "        return []\n",
    "\n",
    "    qvec = np.array(embed_query(query))     # ★ 여기!\n",
    "\n",
    "    desired = extract_tags(query)\n",
    "    by_place = defaultdict(lambda: {\"best_sim\": -1.0, \"sents\": [], \"tags\": set(), \"meta\": None})\n",
    "    for doc, meta, _dist in hits:\n",
    "        place = meta.get(\"place\", \"\")\n",
    "        doc_tags = extract_tags(doc)\n",
    "        sents = split_sentences(doc)\n",
    "        if not sents:\n",
    "            continue\n",
    "        vecs = np.array(embed_passages(sents))  # ★ 여기!\n",
    "        sims = (vecs @ qvec)\n",
    "        top_idx = sims.argsort()[::-1][:max(1, per_place_sent)]\n",
    "        top_sents = [sents[i] for i in top_idx]\n",
    "        top_sim = float(sims[top_idx[0]])\n",
    "        rule_bonus = len(doc_tags & desired)\n",
    "        slot = by_place[place]\n",
    "        if top_sim > slot[\"best_sim\"]:\n",
    "            slot[\"best_sim\"] = top_sim\n",
    "            slot[\"sents\"] = top_sents\n",
    "            slot[\"meta\"] = meta\n",
    "        slot[\"tags\"] |= (doc_tags | desired)\n",
    "        slot[\"rule_bonus\"] = min(3, slot.get(\"rule_bonus\", 0) + rule_bonus)\n",
    "\n",
    "    results = []\n",
    "    for place, slot in by_place.items():\n",
    "        sim = (max(slot[\"best_sim\"], 0.0) + 1.0) / 2.0\n",
    "        rule = (slot.get(\"rule_bonus\", 0)) / 3.0\n",
    "        score = w_sim * sim + w_rule * rule\n",
    "        results.append({\n",
    "            \"place\": place,\n",
    "            \"score\": float(score),\n",
    "            \"sentences\": slot[\"sents\"][:per_place_sent],\n",
    "            \"tags\": sorted(list(slot[\"tags\"])),\n",
    "            \"meta\": slot[\"meta\"],\n",
    "        })\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return results[:top_n]\n",
    "\n",
    "def answer_extractive(question: str, top_k: int = TOP_K, region: str | None = DEFAULT_REGION_FILTER, n_sent: int = 4) -> str:\n",
    "    import numpy as np, re\n",
    "    hits = retrieve(question, top_k=top_k, region=region)\n",
    "    if not hits:\n",
    "        return \"관련 문서를 찾지 못했어요. 다른 키워드로 다시 물어봐 주세요.\"\n",
    "\n",
    "    fields = detect_intents(question)\n",
    "    picked_lines = []\n",
    "    if fields:\n",
    "        for doc, meta, _ in hits:\n",
    "            picked_lines += extract_field_lines(doc, fields)\n",
    "        picked_lines = list(dict.fromkeys(picked_lines))\n",
    "\n",
    "    if len(picked_lines) < n_sent:\n",
    "        query_vec = np.array(embed_query(question))   # ★ 여기!\n",
    "        cand_sents = []\n",
    "        sent_to_place = {}\n",
    "        for doc, meta, _ in hits:\n",
    "            place = meta.get(\"place\",\"\")\n",
    "            sents = split_sentences(doc)\n",
    "            if not sents: \n",
    "                continue\n",
    "            vecs = np.array(embed_passages(sents))    # ★ 여기!\n",
    "            sims = (vecs @ query_vec)\n",
    "            top_idx = sims.argsort()[::-1][:max(2, n_sent)]\n",
    "            for i in top_idx:\n",
    "                sent = sents[i].strip()\n",
    "                if len(sent) >= 6:\n",
    "                    cand_sents.append((sims[i], sent, place))\n",
    "                    sent_to_place[sent] = place\n",
    "\n",
    "        cand_sents.sort(key=lambda x: x[0], reverse=True)\n",
    "        for _, sent, place in cand_sents:\n",
    "            if sent not in picked_lines:\n",
    "                picked_lines.append(sent)\n",
    "            if len(picked_lines) >= n_sent:\n",
    "                break\n",
    "\n",
    "    top_places = [m.get(\"place\",\"\") for _, m, _ in hits if m.get(\"place\")]\n",
    "    top_places = [p for i,p in enumerate(top_places) if p and p not in top_places[:i]]\n",
    "    header = f\"{top_places[0]} 관련 안내예요.\" if top_places else \"\"\n",
    "    sources = \", \".join(top_places[:5])\n",
    "\n",
    "    parts = []\n",
    "    if header: parts.append(header)\n",
    "    parts += picked_lines[:max(3, n_sent)]\n",
    "    if sources: parts.append(f\"(출처: {sources})\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "print(\"패치 적용 완료 ✅  — 새 컬렉션:\", COLLECTION_NAME, \"| DB:\", PERSIST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f5d1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['여긴 산책로가 좋아요.', '사진 찍기 좋고요.', '주차는 가능', '화장실 있음.', '입장료는 무료예요.']\n"
     ]
    }
   ],
   "source": [
    "tmp = \"여긴 산책로가 좋아요. 사진 찍기 좋고요. 주차는 가능 / 화장실 있음. 입장료는 무료예요.\"\n",
    "print(split_sentences(tmp))\n",
    "# ['여긴 산책로가 좋아요.', '사진 찍기 좋고요.', '주차는 가능', '화장실 있음.', '입장료는 무료예요.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec54aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ① 태그 룰 (문서/질문에서 태깅) ======\n",
    "import re, numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# 문서(text)와 질문(query)에서 공통으로 사용할 태깅 규칙 (간단 키워드 기반)\n",
    "TAG_RULES = {\n",
    "    \"실내\": r\"실내|전시|박물관|미술관|갤러리|도서관|아쿠아리움|스파|온천\",\n",
    "    \"카페\": r\"카페|티룸|로스터리|베이커리\",\n",
    "    \"조용\": r\"조용|한적|정숙|휴식|힐링|고즈넉\",\n",
    "    \"산책\": r\"산책|둘레길|숲길|정원|호수|하천|해안산책|트레일\",\n",
    "    \"포토\": r\"사진|포토|전망|뷰|야경|인생샷|포토존\",\n",
    "    \"체험\": r\"체험|프로그램|공방|만들기|시음|시연|투어\",\n",
    "    \"아이\": r\"아기|유아|키즈|아이|어린이|유모차\",\n",
    "    \"부모님\": r\"부모|어르신|노부모|효도|한옥|전통|천천히|완만\",\n",
    "    \"연인\": r\"연인|데이트|감성|분위기\",\n",
    "    \"반려동물\": r\"반려동물|펫|동물동반\",\n",
    "    \"휠체어\": r\"휠체어|무장애|엘리베이터|경사로|장애인\",\n",
    "    \"주차\": r\"주차|주차장\",\n",
    "    \"비오는날\": r\"비\\s*오는|우천|레인|실내\",           # 우천 동선\n",
    "    \"여름\": r\"여름|계곡|폭포|그늘|숲|해수욕장|수영|냉\",\n",
    "    \"겨울\": r\"겨울|온천|스파|따뜻|실내\",\n",
    "    \"봄꽃\": r\"봄꽃|벚꽃|유채꽃|튤립|꽃\",\n",
    "    \"단풍\": r\"단풍|가을\",\n",
    "    \"액티비티\": r\"레저|액티비티|카약|서핑|패러글라이딩|승마|ATV|짚라인|클라이밍|집라인\",\n",
    "}\n",
    "\n",
    "def extract_tags(text: str) -> set[str]:\n",
    "    tags = set()\n",
    "    for tag, pat in TAG_RULES.items():\n",
    "        if re.search(pat, str(text), flags=re.IGNORECASE):\n",
    "            tags.add(tag)\n",
    "    return tags\n",
    "\n",
    "# ====== ② 추천기: 질의 → 상위 문서 회수 → 문장 재랭킹 → 규칙 보너스 → 점수 ======\n",
    "def recommend_places(\n",
    "    query: str,\n",
    "    region: str | None = None,     # 메타 region 필터 (예: \"제주 (전부완료)\")\n",
    "    k_docs: int = 20,              # 먼저 가져올 문서 수\n",
    "    top_n: int = 5,                # 최종 추천 개수\n",
    "    per_place_sent: int = 2,       # 한 장소에서 이유로 보여줄 문장 수\n",
    "    w_sim: float = 0.85,           # 임베딩 유사도 가중치\n",
    "    w_rule: float = 0.15,          # 규칙/태그 보너스 가중치\n",
    "):\n",
    "    # 1) 상위 문서 회수\n",
    "    hits = retrieve(query, top_k=k_docs, region=region)\n",
    "    if not hits:\n",
    "        return []\n",
    "\n",
    "    # 2) 질의/문장 임베딩\n",
    "    qvec = np.array(embed_texts([query])[0])\n",
    "\n",
    "    # 3) 질문에서 희망 태그 추출\n",
    "    desired = extract_tags(query)\n",
    "\n",
    "    # 4) 장소별 스코어링\n",
    "    by_place = defaultdict(lambda: {\"best_sim\": -1.0, \"sents\": [], \"tags\": set(), \"meta\": None})\n",
    "    for doc, meta, _dist in hits:\n",
    "        place = meta.get(\"place\", \"\")\n",
    "        doc_tags = extract_tags(doc)\n",
    "\n",
    "        # 문장 단위 임베딩 & 유사도\n",
    "        sents = split_sentences(doc)\n",
    "        if not sents:\n",
    "            continue\n",
    "        vecs = np.array(embed_texts(sents))\n",
    "        sims = (vecs @ qvec)  # 정규화되어 있으므로 내적=코사인\n",
    "\n",
    "        # 이 문서에서 상위 문장 추출\n",
    "        top_idx = sims.argsort()[::-1][:max(1, per_place_sent)]\n",
    "        top_sents = [sents[i] for i in top_idx]\n",
    "        top_sim = float(sims[top_idx[0]])\n",
    "\n",
    "        # 규칙 보너스: (문서 태그 ∩ 질문 태그) 개수\n",
    "        rule_bonus = len(doc_tags & desired)\n",
    "\n",
    "        # 누적 업데이트(같은 place의 여러 문서가 들어왔을 때 최댓값 유지)\n",
    "        slot = by_place[place]\n",
    "        # 더 높은 유사도면 갈아끼우기\n",
    "        if top_sim > slot[\"best_sim\"]:\n",
    "            slot[\"best_sim\"] = top_sim\n",
    "            slot[\"sents\"] = top_sents\n",
    "            slot[\"meta\"] = meta\n",
    "        # 태그는 합집합\n",
    "        slot[\"tags\"] |= (doc_tags | desired)\n",
    "        # 보너스는 누적 최대 3 정도로 캡핑(과도한 편향 방지)\n",
    "        slot[\"rule_bonus\"] = min(3, slot.get(\"rule_bonus\", 0) + rule_bonus)\n",
    "\n",
    "    # 5) 최종 점수 = w_sim * 유사도 + w_rule * (보너스 정규화)\n",
    "    results = []\n",
    "    for place, slot in by_place.items():\n",
    "        sim = max(slot[\"best_sim\"], 0.0)                  # [-1,1] → [0,1] 보정\n",
    "        sim = (sim + 1.0) / 2.0\n",
    "        rule = (slot.get(\"rule_bonus\", 0)) / 3.0          # 0~1\n",
    "        score = w_sim * sim + w_rule * rule\n",
    "        results.append({\n",
    "            \"place\": place,\n",
    "            \"score\": float(score),\n",
    "            \"sentences\": slot[\"sents\"][:per_place_sent],\n",
    "            \"tags\": sorted(list(slot[\"tags\"])),\n",
    "            \"meta\": slot[\"meta\"],\n",
    "        })\n",
    "\n",
    "    # 6) 점수순 정렬 & 상위 N개\n",
    "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    # 7) (선택) 같은 place 중복 제거는 위에서 place key로 이미 해결됨\n",
    "    return results[:top_n]\n",
    "\n",
    "# ====== ③ 보기 좋게 출력 ======\n",
    "def pretty_print_recos(recos: list[dict]):\n",
    "    for i, r in enumerate(recos, 1):\n",
    "        m = r[\"meta\"] or {}\n",
    "        addr = m.get(\"address\", \"\")\n",
    "        print(f\"{i}. {r['place']}  |  score={r['score']:.3f}\")\n",
    "        if addr:\n",
    "            print(f\"   주소: {addr}\")\n",
    "        if r[\"sentences\"]:\n",
    "            for s in r[\"sentences\"]:\n",
    "                print(f\"   • {s}\")\n",
    "        if r[\"tags\"]:\n",
    "            print(f\"   태그: {', '.join(r['tags'])}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4efb6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingest 경기 (전부완료): 100%|██████████| 13/13 [14:33<00:00, 67.18s/it]\n",
      "Ingest 광주 (전부완료): 100%|██████████| 2/2 [01:29<00:00, 44.56s/it]\n",
      "Ingest 제주 (전부완료): 100%|██████████| 4/4 [04:22<00:00, 65.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[완료] 인덱싱 문서 수: 4257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4257"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingest_csvs(CSV_PATHS)  # 실행 후 count 다시 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4780ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB 폴더: ./chroma_travel_db_e5\n",
      "컬렉션: travel_places_e5\n",
      "문서 수: 4257\n"
     ]
    }
   ],
   "source": [
    "print(\"DB 폴더:\", PERSIST_DIR)\n",
    "print(\"컬렉션:\", COLLECTION_NAME)\n",
    "print(\"문서 수:\", collection.count())  # 0이면 아직 안 들어감\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f0edef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "존재하는 region들: {'경기 (전부완료)'}\n"
     ]
    }
   ],
   "source": [
    "# DB에서 몇 개 메타만 훑어서 region 값 확인\n",
    "sample = collection.get(limit=20, include=[\"metadatas\"])\n",
    "regions = { (m or {}).get(\"region\",\"\") for m in sample.get(\"metadatas\", []) }\n",
    "print(\"존재하는 region들:\", regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6742607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예상 region들: ['경기 (전부완료)', '광주 (전부완료)', '제주 (전부완료)']\n"
     ]
    }
   ],
   "source": [
    "# 인덱싱할 때 저장된 region 문자열은 파일명 stem 그대로야.\n",
    "from pathlib import Path\n",
    "regions_expected = [Path(p).stem for p in CSV_PATHS]\n",
    "print(\"예상 region들:\", regions_expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e142d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 아침미소목장  |  score=0.905\n",
      "   주소: 제주특별자치도 제주시 첨단동길 160-20 (월평동)\n",
      "   • 화장실: 장애인 화장실 있음\n",
      "   • 이용가능시설: 단체석 가능, 무선인터넷 사용가능\n",
      "   태그: 겨울, 부모님, 비오는날, 실내, 아이, 조용, 주차, 체험, 카페, 휠체어\n",
      "\n",
      "2. 헬로키티아일랜드  |  score=0.905\n",
      "   주소: 제주특별자치도 서귀포시 안덕면 한창로 340\n",
      "   • 화장실: 장애인 화장실 있음\n",
      "   • 헬로키티 메모리 룸\n",
      "   태그: 겨울, 비오는날, 실내, 아이, 조용, 주차, 체험, 카페, 휠체어\n",
      "\n",
      "3. 점보빌리지  |  score=0.905\n",
      "   주소: 제주특별자치도 서귀포시 안덕면 평화로319번길 31-11\n",
      "   • 화장실: 장애인 전용 화장실 있음\n",
      "   • 이곳은 코끼리들이 자유롭게 휴식하고 생활하는 공간으로, 코끼리와 사람들이 함께 공존할 수 있도록 조성된 곳이다.\n",
      "   태그: 겨울, 비오는날, 실내, 조용, 주차, 체험, 카페, 휠체어\n",
      "\n",
      "4. 송당 제주살롱  |  score=0.905\n",
      "   주소: 제주특별자치도 제주시 구좌읍 송당2길 7-1\n",
      "   • 북카페 내부에도 혼자 여행하는 이들이 편안하게 책을 읽을 수 있는 공간들을 배려한 느낌이다.\n",
      "   • 간단한 음료와 함께 책을 읽을 수 있는 이곳은 북스테이를 할 수 있는 생각의 오름도 함께 운영한다.\n",
      "   태그: 겨울, 비오는날, 실내, 조용, 주차, 체험, 카페\n",
      "\n",
      "5. 휴림  |  score=0.902\n",
      "   주소: 제주특별자치도 제주시 애월읍 광령남서길 40\n",
      "   • 화장실: 있음\n",
      "   • 주차: 가능\n",
      "   태그: 겨울, 비오는날, 실내, 아이, 여름, 조용, 주차, 체험, 카페\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recos = recommend_places(\"비 오는 날 조용히 쉴 수 있는 실내 카페\", region=\"제주 (전부완료)\", top_n=5)\n",
    "pretty_print_recos(recos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09852db3",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Collection expecting embedding with dimension of 384, got 768",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m recos = \u001b[43mrecommend_places\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m소개팅하기에 좋은 장소 추천해줘\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m경기 (전부완료)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m pretty_print_recos(recos)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mrecommend_places\u001b[39m\u001b[34m(query, region, k_docs, top_n, per_place_sent, w_sim, w_rule)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecommend_places\u001b[39m(\n\u001b[32m     36\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     37\u001b[39m     region: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,     \u001b[38;5;66;03m# 메타 region 필터 (예: \"제주 (전부완료)\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m ):\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# 1) 상위 문서 회수\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     hits = \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m hits:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mretrieve\u001b[39m\u001b[34m(query, top_k, region)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m region:\n\u001b[32m    152\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mwhere\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33mregion\u001b[39m\u001b[33m\"\u001b[39m: region}\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m res = \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m docs = res[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m res.get(\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    155\u001b[39m metas = res[\u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m res.get(\u001b[33m\"\u001b[39m\u001b[33mmetadatas\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\skn17\\anaonda\\envs\\llm_env\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:225\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    190\u001b[39m \n\u001b[32m    191\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    210\u001b[39m \n\u001b[32m    211\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m query_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_query_request(\n\u001b[32m    214\u001b[39m     query_embeddings=query_embeddings,\n\u001b[32m    215\u001b[39m     query_texts=query_texts,\n\u001b[32m   (...)\u001b[39m\u001b[32m    222\u001b[39m     include=include,\n\u001b[32m    223\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m query_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere_document\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(\n\u001b[32m    238\u001b[39m     response=query_results, include=query_request[\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    239\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\skn17\\anaonda\\envs\\llm_env\\Lib\\site-packages\\chromadb\\api\\rust.py:519\u001b[39m, in \u001b[36mRustBindingsAPI._query\u001b[39m\u001b[34m(self, collection_id, query_embeddings, ids, n_results, where, where_document, include, tenant, database)\u001b[39m\n\u001b[32m    503\u001b[39m filtered_ids_amount = \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    505\u001b[39m     CollectionQueryEvent(\n\u001b[32m    506\u001b[39m         collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    516\u001b[39m     )\n\u001b[32m    517\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m rust_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\n\u001b[32m    532\u001b[39m     ids=rust_response.ids,\n\u001b[32m    533\u001b[39m     embeddings=rust_response.embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m     distances=rust_response.distances,\n\u001b[32m    540\u001b[39m )\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Collection expecting embedding with dimension of 384, got 768"
     ]
    }
   ],
   "source": [
    "recos = recommend_places(\"소개팅하기에 좋은 장소 추천해줘\", region=\"경기 (전부완료)\", top_n=5)\n",
    "pretty_print_recos(recos)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
